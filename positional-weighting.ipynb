{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTTEXT_PARAMETERS = {\n",
    "    'sg': 0,\n",
    "    'bucket': 2 * 10**6,\n",
    "    'negative': 10,\n",
    "    'alpha': 0.05,\n",
    "    'min_alpha': 0,\n",
    "    'sample': 10**-5,\n",
    "    'min_n': 3,\n",
    "    'max_n': 6,\n",
    "    'min_count': 5,\n",
    "    'workers': 32,\n",
    "    'epochs': 1,\n",
    "    'vector_size': 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "WITIKO_URL = 'https://github.com/Witiko/gensim.git'\n",
    "\n",
    "REPOSITORIES = {\n",
    "    'Branch position-dependent-weighting-vanilla': (WITIKO_URL, 'efe3126', {}),\n",
    "    'Branch position-dependent-weighting-identity': (WITIKO_URL, '94a57ff', {}),\n",
    "    'Branch position-dependent-weighting-uniform': (WITIKO_URL, 'fa9dfcf', {}),\n",
    "    'Branch position-dependent-weighting-square-normal': (WITIKO_URL, '84291c1', {}),\n",
    "}\n",
    "\n",
    "CONFIGURATIONS = {\n",
    "    'CBOW+NS': {},\n",
    "    'CBOW+NS+PDW': {'position_dependent_weights': 1},\n",
    "}\n",
    "\n",
    "VARIANTS = {\n",
    "    'Window size 5': {'window': 5},\n",
    "    'Window size 15': {'window': 15},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.3\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_US.UTF-8'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, ('en_US', 'UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blas_mkl_info:\n",
      "  NOT AVAILABLE\n",
      "blis_info:\n",
      "  NOT AVAILABLE\n",
      "openblas_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "blas_opt_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "lapack_mkl_info:\n",
      "  NOT AVAILABLE\n",
      "openblas_lapack_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n",
      "lapack_opt_info:\n",
      "    libraries = ['openblas', 'openblas']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.show_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_ANALOGY_DATASET_FILENAME = 'data/word-analogies/questions-words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -e -c 'LC_ALL=C bash'\n",
    "set -e\n",
    "if ! [ -e data/word-analogies ]\n",
    "then\n",
    "    mkdir -p data/word-analogies\n",
    "    cd data/word-analogies\n",
    "    wget https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "PHRASERS = tuple()\n",
    "\n",
    "def _read_sentences_helper(article):\n",
    "    from gensim.utils import simple_preprocess\n",
    "    all_sentences = []\n",
    "    for section_title, section_text in zip(article['section_titles'], article['section_texts']):\n",
    "        sentences = section_text.splitlines()\n",
    "        sentences = map(str.strip, sentences)\n",
    "        sentences = (sentence for sentence in sentences if sentence)\n",
    "        sentences = chain([section_title], sentences)\n",
    "        sentences = map(simple_preprocess, sentences)\n",
    "        all_sentences.extend(sentences)\n",
    "    for phraser in PHRASERS:\n",
    "        all_sentences = phraser[all_sentences]\n",
    "    all_sentences = list(all_sentences)\n",
    "    return all_sentences\n",
    "\n",
    "class CorpusSentenceIterator(object):\n",
    "    def __init__(self, phrasers=tuple(), corpus_name='wiki-english-20171001', percentage=1.0):\n",
    "        self.phrasers = tuple(phrasers)\n",
    "        self.corpus_name = corpus_name\n",
    "        self.percentage = percentage\n",
    "        self.iterable = None\n",
    "        import gensim.downloader\n",
    "        gensim.downloader.load(self.corpus_name)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.__init__(self.phrasers, self.corpus_name, self.percentage)\n",
    "        return self\n",
    "\n",
    "    def _read_sentences(self, corpus_num_articles=4924894):\n",
    "        assert CURRENT_GENSIM_URL_AND_REF is not None  # _reinstall_gensim has been called prior to importing gensim\n",
    "        import gensim.downloader\n",
    "        from tqdm import tqdm\n",
    "        total = int(corpus_num_articles * self.percentage)\n",
    "        desc = 'Reading articles from the {} corpus'.format(self.corpus_name)\n",
    "        articles = gensim.downloader.load(self.corpus_name)\n",
    "        articles = (article for article, _ in zip(articles, range(total)))\n",
    "        articles = tqdm(articles, desc=desc, total=total)\n",
    "        global PHRASERS\n",
    "        PHRASERS = self.phrasers\n",
    "        from multiprocessing import Pool\n",
    "        with Pool(None) as pool:\n",
    "            for sentences in pool.imap_unordered(_read_sentences_helper, articles):\n",
    "                for sentence in sentences:\n",
    "                    yield sentence            \n",
    "\n",
    "    def __next__(self):\n",
    "        if self.iterable is None:\n",
    "            self.iterable = self._read_sentences()\n",
    "        corpus_sentence = next(self.iterable)\n",
    "        return corpus_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_GENSIM_URL_AND_REF = None\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "def _reinstall_gensim(url, ref):\n",
    "    global CURRENT_GENSIM_URL_AND_REF\n",
    "    if CURRENT_GENSIM_URL_AND_REF == (url, ref):\n",
    "        return  # only reinstall when necessary\n",
    "    from sys import modules\n",
    "    if 'gensim' in modules:\n",
    "        raise RuntimeError('Restart the kernel and rerun the Jupyter notebook')\n",
    "    ! pip uninstall gensim -y\n",
    "    ! cd /var/tmp && \\\n",
    "      rm -rf gensim && \\\n",
    "      git clone {url} && \\\n",
    "      cd gensim && \\\n",
    "      git checkout {ref} && \\\n",
    "      TMPDIR=/var/tmp pip install . && \\\n",
    "      python setup.py build_ext --inplace && \\\n",
    "      python -c 'from gensim.models import FastText' && \\\n",
    "      cd .. && \\\n",
    "      rm -rf gensim\n",
    "    CURRENT_GENSIM_URL_AND_REF = (url, ref)\n",
    "\n",
    "def _stringify_parameters(parameters):\n",
    "    parameters = sorted(parameters.items())\n",
    "    import re\n",
    "    def stringify(obj): return re.sub('_', '-', str(obj))\n",
    "    parameters = ('{}={}'.format(stringify(key), stringify(value)) for key, value in parameters if key != 'url')\n",
    "    parameters = '_'.join(parameters)\n",
    "    return parameters\n",
    "\n",
    "def _format_duration(duration):\n",
    "    hours = int(duration // 3600)\n",
    "    minutes = int((duration % 3600) // 60)\n",
    "    seconds = int(round((duration % 60)))\n",
    "    return '{:2d}h {:02d}m {:02d}s'.format(hours, minutes, seconds)\n",
    "\n",
    "def _format_accuracy(accuracy):\n",
    "    return '{:.2f}%'.format(100.0 * accuracy)\n",
    "\n",
    "def _phrasers(num_iterations=0, starting_threshold=100.0, final_threshold=100.0):\n",
    "    from gensim.models.phrases import Phraser\n",
    "    phrasers = []\n",
    "    phrases_filename_template = 'data/models/phrases_{}'\n",
    "    phraser_filename_template = 'data/models/phraser_{}'\n",
    "    for iteration in range(num_iterations):\n",
    "        progress = iteration * 1.0 / (num_iterations - 1) if num_iterations > 1 else 0.0\n",
    "        threshold = starting_threshold + progress * (final_threshold - starting_threshold)\n",
    "        parameters = {'iteration': iteration + 1, 'threshold': threshold}\n",
    "        stringified_parameters = _stringify_parameters(parameters)\n",
    "        phrases_filename = phrases_filename_template.format(stringified_parameters)\n",
    "        phraser_filename = phraser_filename_template.format(stringified_parameters)\n",
    "        try:\n",
    "            phraser = Phraser.load(phraser_filename)\n",
    "        except IOError as e:\n",
    "            from gensim.models.phrases import Phrases\n",
    "            sentences = CorpusSentenceIterator(phrasers=phrasers)\n",
    "            phrases_kwargs = {\n",
    "                'sentences': sentences,\n",
    "                'max_vocab_size': float('inf'),\n",
    "                'threshold': threshold,\n",
    "            }\n",
    "            phrases = Phrases(**phrases_kwargs)\n",
    "            phraser = Phraser(phrases)\n",
    "            ! mkdir -p data/models\n",
    "            phrases.save(phrases_filename)\n",
    "            phraser.save(phraser_filename)\n",
    "        phrasers.append(phraser)\n",
    "    return phrasers\n",
    "\n",
    "def _train(fasttext_model_filename, fasttext_parameters, only_load=False):\n",
    "    training_duration_filename = '{}.duration'.format(fasttext_model_filename)\n",
    "    try:\n",
    "        with open(training_duration_filename, 'rt') as f:\n",
    "            training_duration = float(f.read())\n",
    "    except IOError as e:\n",
    "        if only_load:\n",
    "            raise e\n",
    "        from datetime import datetime\n",
    "        assert CURRENT_GENSIM_URL_AND_REF is not None  # # _reinstall_gensim has been called prior to importing gensim\n",
    "        from gensim.models.fasttext import FastText\n",
    "        corpus_sentences = CorpusSentenceIterator(phrasers=_phrasers())\n",
    "        training_start_time = datetime.now()\n",
    "        fasttext_model = FastText(corpus_sentences, **fasttext_parameters)\n",
    "        training_finish_time = datetime.now()\n",
    "        training_duration = (training_finish_time - training_start_time).total_seconds()\n",
    "        ! mkdir -p data/models\n",
    "        fasttext_model.save(fasttext_model_filename)\n",
    "        with open(training_duration_filename, 'wt') as f:\n",
    "            print(training_duration, file=f)\n",
    "    return training_duration\n",
    "\n",
    "def _evaluate(fasttext_model_filename, word_analogy_dataset_filename, only_load=False):\n",
    "    word_analogy_accuracy_filename = '{}.accuracy'.format(fasttext_model_filename)\n",
    "    try:\n",
    "        with open(word_analogy_accuracy_filename, 'rt') as f:\n",
    "            word_analogy_accuracy = float(f.read())\n",
    "    except IOError:\n",
    "        if only_load:\n",
    "            raise e\n",
    "        assert CURRENT_GENSIM_URL_AND_REF is not None  # # _reinstall_gensim has been called prior to importing gensim\n",
    "        from gensim.models.fasttext import FastText\n",
    "        fasttext_model = FastText.load(fasttext_model_filename)\n",
    "        def evaluate(*args, **kwargs): return fasttext_model.wv.evaluate_word_analogies(*args, **kwargs)\n",
    "        word_analogy_accuracy, _ = evaluate(word_analogy_dataset_filename, restrict_vocab=200000)\n",
    "        with open(word_analogy_accuracy_filename, 'wt') as f:\n",
    "            print(word_analogy_accuracy, file=f)\n",
    "    return word_analogy_accuracy\n",
    "\n",
    "def _train_fasttext_model(install_parameters, fasttext_parameters, fasttext_model_filename):\n",
    "    train_parameters = (fasttext_model_filename, fasttext_parameters)\n",
    "    evaluate_parameters = (fasttext_model_filename, WORD_ANALOGY_DATASET_FILENAME)\n",
    "    \n",
    "    try:\n",
    "        training_duration = _train(*train_parameters, only_load=True)\n",
    "    except IOError:\n",
    "        _reinstall_gensim(**install_parameters)\n",
    "        training_duration = _train(*train_parameters)\n",
    "        word_analogy_accuracy = _evaluate(*evaluate_parameters)\n",
    "    else:\n",
    "        try:\n",
    "            word_analogy_accuracy = _evaluate(*evaluate_parameters, only_load=True)\n",
    "        except IOError:\n",
    "            _reinstall_gensim(**install_parameters)\n",
    "            word_analogy_accuracy = _evaluate(*evaluate_parameters)\n",
    "    \n",
    "    return (training_duration, word_analogy_accuracy)\n",
    "\n",
    "def _resolve_text_specification(repository, configuration, variant, **call_fasttext_parameters):\n",
    "    configuration_fasttext_parameters = CONFIGURATIONS[configuration]\n",
    "    url, ref, repository_fasttext_parameters = REPOSITORIES[repository]\n",
    "    variant_fasttext_parameters = VARIANTS[variant]\n",
    "    fasttext_parameters = {\n",
    "        **FASTTEXT_PARAMETERS,\n",
    "        **configuration_fasttext_parameters,\n",
    "        **repository_fasttext_parameters,\n",
    "        **variant_fasttext_parameters,\n",
    "        **call_fasttext_parameters,\n",
    "    }\n",
    "    install_parameters = {'url': url, 'ref': ref}\n",
    "    parameter_string = _stringify_parameters({**fasttext_parameters, **install_parameters})\n",
    "    fasttext_model_filename = 'data/models/fasttext-model_{}'.format(parameter_string)\n",
    "    return (install_parameters, fasttext_parameters, fasttext_model_filename)\n",
    "\n",
    "def train_fasttext_model(*args, **kwargs):\n",
    "    training_duration, word_analogy_accuracy = _train_fasttext_model(*_resolve_text_specification(*args, **kwargs))\n",
    "    print('Training duration: {}'.format(_format_duration(training_duration)))\n",
    "    print('English word analogy task, total accuracy: {}'.format(_format_accuracy(word_analogy_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training duration:  2h 06m 33s\n",
      "English word analogy task, total accuracy: 65.52%\n"
     ]
    }
   ],
   "source": [
    "train_fasttext_model('Branch position-dependent-weighting-identity', 'CBOW+NS', 'Window size 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training duration:  4h 41m 17s\n",
      "English word analogy task, total accuracy: 70.94%\n"
     ]
    }
   ],
   "source": [
    "train_fasttext_model('Branch position-dependent-weighting-identity', 'CBOW+NS', 'Window size 5', epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training duration:  5h 01m 16s\n",
      "English word analogy task, total accuracy: 50.96%\n"
     ]
    }
   ],
   "source": [
    "train_fasttext_model('Branch position-dependent-weighting-vanilla', 'CBOW+NS+PDW', 'Window size 15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training duration:  4h 59m 27s\n",
      "English word analogy task, total accuracy: 75.02%\n"
     ]
    }
   ],
   "source": [
    "train_fasttext_model('Branch position-dependent-weighting-identity', 'CBOW+NS+PDW', 'Window size 15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training duration:  4h 57m 25s\n",
      "English word analogy task, total accuracy: 74.31%\n"
     ]
    }
   ],
   "source": [
    "train_fasttext_model('Branch position-dependent-weighting-uniform', 'CBOW+NS+PDW', 'Window size 15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training duration:  5h 01m 11s\n",
      "English word analogy task, total accuracy: 74.95%\n"
     ]
    }
   ],
   "source": [
    "train_fasttext_model('Branch position-dependent-weighting-square-normal', 'CBOW+NS+PDW', 'Window size 15')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
